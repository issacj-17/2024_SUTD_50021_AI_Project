{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "180a1aec5ed440f0b208413d7943167a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 57,
    "execution_start": 1711448586528,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cell_id": "e0635c35eadc4ede891c3166bb960ad2",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 129,
    "execution_start": 1711450005400,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TspDataset(Dataset):\n",
    "    def __init__(self, split = \"train\"):\n",
    "        super().__init__()\n",
    "        data_root = Path(\"../data/processed_heuristic-threshold\")\n",
    "        instances = data_root / \"instances\" / \"1\"\n",
    "        instances = sum((list(instance.iterdir()) for instance in instances.iterdir() if instance.stem != \"1\"), [])\n",
    "\n",
    "        train_instances, val_instances = train_test_split(instances, test_size = 0.2, random_state = 42)\n",
    "        self.instances = train_instances if split == \"train\" else val_instances\n",
    "    \n",
    "    def get(self, idx):\n",
    "        entry = self.instances[idx]\n",
    "        with np.load(entry / \"pairwise.npz\") as data:\n",
    "            distance_matrix = torch.tensor(data['arr_0'], dtype=torch.float)\n",
    "        distance_matrix = distance_matrix / distance_matrix.max()\n",
    "        route_mask = torch.from_numpy(np.loadtxt(entry / \"sol_mask.txt\", dtype=np.float32))[:-1, :-1]\n",
    "        route_mask = route_mask + route_mask.mT\n",
    "        \n",
    "        # route_distance = data['route_distance']\n",
    "        # distance_matrix = distance_matrix.cuda()\n",
    "        # route_mask = route_mask.cuda()\n",
    "\n",
    "        graph = Data(x=distance_matrix[:, :1], edge_attr=distance_matrix, y=route_mask)\n",
    "        return graph\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cell_id": "26131966b1b3442f87b4f36bea914e67",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 61,
    "execution_start": 1711449390606,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from einops import rearrange\n",
    "\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.activation_fn = nn.ReLU()\n",
    "#         self.graph_conv = gnn.DenseGATConv(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, graph):\n",
    "#         node_feats = self.graph_conv(graph.x, graph.edge_attr)\n",
    "#         node_feats = self.activation_fn(node_feats)\n",
    "\n",
    "#         graph.x = node_feats\n",
    "#         return graph\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.activation_fn = nn.ReLU()\n",
    "\n",
    "        self.node_conv = gnn.DenseGATConv(input_dim, output_dim)\n",
    "        \n",
    "        self.node_edge_conv = gnn.DenseGATConv(input_dim, output_dim)\n",
    "        self.edge_value_conv = nn.Conv2d(output_dim, 1, 1)\n",
    "        self.edge_edge_conv = nn.Conv2d(input_dim, output_dim, 1)\n",
    "    \n",
    "    def forward(self, graph):    \n",
    "        node_feats, edge_values, edge_feats = graph\n",
    "        \n",
    "        new_node_feats = self.node_conv(node_feats, edge_values)\n",
    "        \n",
    "        node_edge_feats = self.node_edge_conv(node_feats, edge_values)\n",
    "        edge_edge_feats = self.edge_edge_conv(edge_feats)\n",
    "       \n",
    "        node_edge_feats = rearrange(node_edge_feats, \"b n c -> b c n 1\") + rearrange(node_edge_feats, \"b n c -> b c 1 n\")\n",
    "\n",
    "        new_edge_feats = node_edge_feats + edge_edge_feats\n",
    "        \n",
    "        new_edge_values = torch.sigmoid(self.edge_value_conv(new_edge_feats))\n",
    "        new_edge_values = rearrange(new_edge_values, \"b 1 n1 n2 -> b n1 n2\")\n",
    "\n",
    "        new_node_feats = self.activation_fn(new_node_feats)\n",
    "        new_edge_feats = self.activation_fn(new_edge_feats)\n",
    "\n",
    "        return (new_node_feats, new_edge_values, new_edge_feats)\n",
    "\n",
    "        # node_feats2 = self.graph_conv2(graph.x, graph.edge_attr)\n",
    "        # node_feats2 = self.activation_fn(node_feats2)\n",
    "        \n",
    "        # edge_feats1 = rearrange(node_feats1, \"b n c -> b c n 1\")\n",
    "        # edge_feats2 = rearrange(node_feats2, \"b n c -> b c 1 n\")\n",
    "        \n",
    "        # edge_feats = self.edge_conv1(edge_feats1) + self.edge_conv2(edge_feats2)\n",
    "        # # edge_feats = edge_feats1 @ edge_feats2\n",
    "\n",
    "        # # gate = self.gate_linear(node_feats.mean(-2))\n",
    "        # # gate = torch.sigmoid(gate)\n",
    "        # # edge_feats = gate * edge_feats + (1 - gate) * graph.edge_attr\n",
    "\n",
    "        # node_feats = node_feats1 + node_feats2# + graph.x.mean(-1, keepdim=True)\n",
    "        # edge_feats = edge_feats# + graph.edge_attr\n",
    "\n",
    "        # return Data(x = node_feats, edge_attr = edge_feats)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            ConvBlock(1, 32),\n",
    "            ConvBlock(32, 32),\n",
    "            ConvBlock(32, 32),\n",
    "            ConvBlock(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, graph):\n",
    "        node_feats, edge_feats = graph.x, graph.edge_attr\n",
    "        node_feats, edge_feats = node_feats.unsqueeze(0), edge_feats.unsqueeze(0)\n",
    "        graph = (node_feats, edge_feats, edge_feats.unsqueeze(1))\n",
    "        _, _, edge_feats = self.model(graph)\n",
    "\n",
    "        out = rearrange(edge_feats, \"1 1 n1 n2 -> n1 n2\")\n",
    "\n",
    "        # graph.edge_attr = torch.softmax(graph.edge_attr, -1)\n",
    "        # out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b1b445ba5900423daab04019a6e5f6a1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1127561,
    "execution_start": 1711450330354,
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 52.39it/s, loss=4.86, std=0.6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: 4.587519645690918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 52.47it/s, loss=4.43, std=0.844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: 4.195655345916748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 52.28it/s, loss=4.09, std=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: 3.953949213027954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 52.24it/s, loss=3.93, std=1.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: 3.8702948093414307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 52.66it/s, loss=3.86, std=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: 3.8247079849243164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 51.76it/s, loss=3.81, std=1.35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: 3.853071689605713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 1080/1080 [00:21<00:00, 51.33it/s, loss=3.78, std=1.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: 3.7317054271698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 51.49it/s, loss=3.75, std=1.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: 3.726921319961548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:21<00:00, 49.85it/s, loss=3.74, std=1.45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: 3.751734495162964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 52.42it/s, loss=3.72, std=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: 3.6599910259246826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 51.43it/s, loss=3.7, std=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: 3.7452828884124756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 51.66it/s, loss=3.7, std=1.55]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: 3.6534581184387207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:21<00:00, 50.92it/s, loss=3.68, std=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12: 3.693665027618408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:20<00:00, 52.02it/s, loss=3.68, std=1.58]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13: 3.6370913982391357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:21<00:00, 50.72it/s, loss=3.67, std=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14: 3.636951446533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 1080/1080 [00:19<00:00, 54.08it/s, loss=3.67, std=1.64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15: 3.7290799617767334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████▎           | 863/1080 [00:17<00:04, 54.07it/s, loss=3.64, std=1.66]"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_dataset = TspDataset()\n",
    "val_dataset = TspDataset(split = \"val\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False, pin_memory=True)\n",
    "# loss_fn = nn.BCELoss(reduction = 'none')\n",
    "# loss_fn = nn.BCEWithLogitsLoss(reduction = 'none')\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def dice_loss(y_pred, y_true):\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    # Flatten the predictions and ground truth\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = y_pred.flatten()\n",
    "\n",
    "    # Compute the intersection and union\n",
    "    intersection = (y_true_flat * y_pred_flat).sum() + 1\n",
    "    union = (y_true_flat).sum() + (y_pred_flat).sum() + 1\n",
    "\n",
    "    # Compute the Dice loss\n",
    "    dice_loss = 1 - 2 * intersection / union\n",
    "\n",
    "    return dice_loss\n",
    "\n",
    "# loss_fn = dice_loss\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "while True:\n",
    "    lowest_val_loss = float(\"inf\")\n",
    "    patience = 0\n",
    "    loss_arr = []\n",
    "    std_arr = []\n",
    "    model = Model().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(200):\n",
    "        if patience >= 3:\n",
    "            continue\n",
    "        cu_loss = None\n",
    "        cu_std = None\n",
    "        pbar = tqdm(train_dataloader, delay=1)\n",
    "        for i, batch in enumerate(pbar):\n",
    "            graph = batch.to(device)\n",
    "            out = model(graph)\n",
    "        \n",
    "            loss = loss_fn(out, graph.y)\n",
    "    \n",
    "            # loss = loss * graph.y\n",
    "            loss = loss.mean()\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            loss = loss.detach().cpu().item()\n",
    "            loss_arr.append(loss)\n",
    "            cu_loss = cu_loss + loss if cu_loss is not None else loss\n",
    "\n",
    "            std = torch.std(out)\n",
    "            std = std.detach().cpu().item()\n",
    "            std_arr.append(std)\n",
    "            cu_std = cu_std + std if cu_std is not None else std\n",
    "    \n",
    "            pbar.set_postfix(loss = cu_loss / (i + 1), std = cu_std / (i + 1))\n",
    "\n",
    "            if i > 10 and (cu_std / (i + 1) < 1e-4):\n",
    "                break\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                cu_loss = None\n",
    "                for batch in val_dataloader:\n",
    "                    graph = batch.to(device)\n",
    "                    out = model(graph)\n",
    "        \n",
    "                    loss = loss_fn(out, graph.y)\n",
    "                    loss = loss.mean()\n",
    "                    cu_loss = cu_loss + loss if cu_loss is not None else loss\n",
    "            val_loss = cu_loss / len(val_dataloader)\n",
    "            print(f\"epoch {epoch}: {val_loss}\")\n",
    "            if val_loss < lowest_val_loss:\n",
    "                patience = 0\n",
    "                lowest_val_loss = val_loss\n",
    "            else:\n",
    "                patience += 1\n",
    "            continue\n",
    "        break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tsp_dataset[11]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    # plt.imshow(graph.edge_attr.squeeze())\n",
    "    # plt.show()\n",
    "    out = model(graph)\n",
    "    plt.imshow(out)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    plt.imshow(graph.y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_arr)\n",
    "plt.show()\n",
    "plt.plot(std_arr)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "48b93839da76417995c0a83e909302b6",
  "deepnote_persisted_session": {
   "createdAt": "2024-03-26T12:15:44.583Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
